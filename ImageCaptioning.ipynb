{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XMSnQoYF-W"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vu56tYtYqQ0",
        "outputId": "4769163e-0d20-4746-bf7d-801358cb1eab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjf6GNIKYx4K"
      },
      "source": [
        "wd = 'My Drive/CV_Project/'\n",
        "\n",
        "with open(os.path.join(wd, 'train_img_id_comments.pkl'),'rb') as f:\n",
        "  train = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(wd, 'train_img_encoding.pkl'),'rb') as f:\n",
        "  train_img_encoding = pickle.load(f)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXar1kzEh-aJ",
        "outputId": "5139b378-5bc1-4723-c105-ba75e3ccbfa3"
      },
      "source": [
        "#adding startsequence and end sequence tokens before and after each caption\n",
        "for id in tqdm(train.keys()):\n",
        "  comments = train[id]\n",
        "  t = []\n",
        "  for comment in comments:\n",
        "    c = '<s> ' + comment + ' </s>'\n",
        "    t.append(c)\n",
        "\n",
        "  train[id] = t"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20340/20340 [00:00<00:00, 333468.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd24Jdq9icgA",
        "outputId": "c25c104c-acf8-4843-fbf9-30550ab8245e"
      },
      "source": [
        "train_images = list(train_img_encoding.keys())\n",
        "train[train_images[100]]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> two indian students looking at book with adults beside them </s>',\n",
              " '<s> indian students working and in discussion in classroom </s>',\n",
              " '<s> two darkhaired dark complected talking </s>',\n",
              " '<s> two indian women treading to each other </s>',\n",
              " '<s> four indian people in classroom </s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DRQViAZpsG"
      },
      "source": [
        "#### Building vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzjKVPs-ZBdL",
        "outputId": "2490a006-7660-4192-db18-a58bc1542e6e"
      },
      "source": [
        "vocab = {}\n",
        "for id in train.keys():\n",
        "    comments = train[id]\n",
        "    for comment in comments:\n",
        "        words = comment.split(' ')\n",
        "        for word in words:\n",
        "            vocab[word] = vocab.get(word, 0) + 1\n",
        "                \n",
        "print (\"Number of unique words in train corpus = %d\" %(len(vocab.keys())))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words in train corpus = 16508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjGDEYiaaScr"
      },
      "source": [
        "Since the number of unique words is too large, we consider only those words which have over 30 occurences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvP4_XEAZhcu",
        "outputId": "c02cb20c-203e-4c20-e0d4-d60267185b51"
      },
      "source": [
        "words = [word for word in vocab.keys() if vocab[word]>30]\n",
        "len(words)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frYYWizJbh4e",
        "outputId": "0a8f76b3-6260-4059-f28b-1dc4f1bf0b80"
      },
      "source": [
        "vocab_words = set(words)\n",
        "#vocab_words.add('<unk>') #token for out vocabulary word\n",
        "vocab_words.add('<pad>') #token for padding word\n",
        "print (len(vocab_words)) "
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGMdvFKhcGc3"
      },
      "source": [
        "word2idx = {}\n",
        "idx2word = {}\n",
        "\n",
        "word2idx['<pad>'] = 0\n",
        "idx2word[0] = '<pad>'\n",
        "\n",
        "idx = 1\n",
        "for word in vocab_words :\n",
        "  if word!='<pad>':\n",
        "    word2idx[word] = idx\n",
        "    idx2word[idx] = word\n",
        "    idx+=1\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYUXTxcicmae",
        "outputId": "a7d9d466-7ebd-4130-ef4d-55525f3219bd"
      },
      "source": [
        "max_len = 0\n",
        "comment_lengths = []\n",
        "\n",
        "for img_id in tqdm(train.keys()):\n",
        "    for comment in train[img_id]:\n",
        "        l = len(comment.split(' '))\n",
        "        if l>max_len:\n",
        "            max_len = l\n",
        "        comment_lengths.append(l)\n",
        "        \n",
        "max_len"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20340/20340 [00:00<00:00, 162796.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLPtUZdJAj"
      },
      "source": [
        "### Building data generator\n",
        "\n",
        "The model will be predicting one word of the caption at a time given the previous words upto that time for the caption and the image encoding as the input. As each image has 5 captions, atleast 5-6 words per caption and there are around 20K train images, the data won't fit in memory while training. So, the data has to be loaded in batches and for that a generator function is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp-Nanzuc2EJ"
      },
      "source": [
        "\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(data, attribute_vec, word2idx, max_len, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in data.items():\n",
        "            n+=1\n",
        "            # retrieve the image feature vector\n",
        "            photo = attribute_vec[key]\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [word2idx[word] for word in desc.split(' ') if word in word2idx]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_len)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            # yield the batch data\n",
        "            if n==num_photos_per_batch:\n",
        "                yield [[array(X1), array(X2)], array(y)]\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6SCNqAUfCUc"
      },
      "source": [
        "datagen = data_generator(train, train_img_encoding, word2idx, max_len, num_photos_per_batch=1)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ureyXitYfRS9",
        "outputId": "86833aa9-4639-4b13-d080-c8b4b10616ba"
      },
      "source": [
        "X, target_word = next(datagen)\n",
        "img_encoding = X[0]\n",
        "partial_caption = X[1]\n",
        "print (img_encoding.shape, partial_caption.shape, target_word.shape)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(38, 2048) (38, 70) (38, 2200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgnF9qzkN4N"
      },
      "source": [
        "### Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zreCrXHujqqW"
      },
      "source": [
        "\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(os.path.join(wd, 'glove.6B.100d.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrFi4g4toV_x"
      },
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = len(word2idx)\n",
        "# Get 100-dim dense vector for each of the words in our vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da80e5zUpEdl"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9e8xNHmoqGa"
      },
      "source": [
        "# image feature extractor model\n",
        "inputs1 = Input(shape=(2048,))\n",
        "# partial caption sequence model\n",
        "inputs2 = Input(shape=(max_len,))\n",
        "\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder (feed forward) model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "# merge the two input models\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glMP7lZ1pVJx",
        "outputId": "70397d61-f9fa-4f53-ea9e-4f3cd41a475e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 70, 100)      220000      input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 2048)         0           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 70, 100)      0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 256)          524544      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 256)          365568      dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 256)          0           dense_12[0][0]                   \n",
            "                                                                 lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 256)          65792       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 2200)         565400      dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,741,304\n",
            "Trainable params: 1,741,304\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqSilnmXpnP_"
      },
      "source": [
        "Freezing the weights of Embedding layer as we don't want to retrain our word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvt9Ktfgpbu0",
        "outputId": "826a742b-9bfa-43c3-ade3-4291f6634acb"
      },
      "source": [
        "model.layers[2]"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f5c701a44d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keorHZ98plmJ"
      },
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu3aXhKYpv6i"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNGRLokEp0Gn"
      },
      "source": [
        "### Training\n",
        "\n",
        "Trying out on a 50 samples of train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjM76vtT5rfl"
      },
      "source": [
        "sample_train = {}\n",
        "for img in train_images[:50]:\n",
        "  sample_train[img] = train[img]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-7l5ubVpyQ7"
      },
      "source": [
        "epochs = 1\n",
        "number_pics_per_batch = 5\n",
        "steps = len(sample_train)//number_pics_per_batch"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez-NENT7p3C9",
        "outputId": "19274107-845d-46ad-a5ca-5580490e8e24"
      },
      "source": [
        "generator = data_generator(sample_train, train_img_encoding, word2idx, max_len, 30)\n",
        "for e in range(epochs):\n",
        "    print('Epoch', e)\n",
        "    batches = 0\n",
        "    for batch_id in range(steps):\n",
        "      print (\"Bacth\", (batch_id+1))\n",
        "      X,y = next(generator)\n",
        "      print (y.shape)\n",
        "      model.fit(X,y)\n",
        "      batches += number_pics_per_batch\n",
        "      if batches >= len(sample_train):\n",
        "        break"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Bacth 1\n",
            "(1674, 2200)\n",
            "53/53 [==============================] - 20s 283ms/step - loss: 6.5675\n",
            "Bacth 2\n",
            "(1706, 2200)\n",
            "54/54 [==============================] - 15s 282ms/step - loss: 5.4662\n",
            "Bacth 3\n",
            "(1673, 2200)\n",
            "53/53 [==============================] - 15s 282ms/step - loss: 4.8739\n",
            "Bacth 4\n",
            "(1743, 2200)\n",
            "55/55 [==============================] - 15s 276ms/step - loss: 4.6830\n",
            "Bacth 5\n",
            "(1670, 2200)\n",
            "53/53 [==============================] - 15s 277ms/step - loss: 4.3752\n",
            "Bacth 6\n",
            "(1674, 2200)\n",
            "53/53 [==============================] - 15s 274ms/step - loss: 3.9705\n",
            "Bacth 7\n",
            "(1706, 2200)\n",
            "54/54 [==============================] - 15s 276ms/step - loss: 3.8945\n",
            "Bacth 8\n",
            "(1673, 2200)\n",
            "53/53 [==============================] - 15s 277ms/step - loss: 3.5199\n",
            "Bacth 9\n",
            "(1743, 2200)\n",
            "55/55 [==============================] - 15s 274ms/step - loss: 3.4882\n",
            "Bacth 10\n",
            "(1670, 2200)\n",
            "53/53 [==============================] - 15s 275ms/step - loss: 3.1822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx2sB_g64iCT"
      },
      "source": [
        "def greedySearch(photo):\n",
        "    #photo = train_img_encoding[photo_id]\n",
        "    photo  = photo[np.newaxis,...]\n",
        "    in_text = '<s>'\n",
        "    for i in range(max_len):\n",
        "        sequence = [word2idx[w] for w in in_text.split() if w in word2idx]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_len)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = idx2word[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == '</s>':\n",
        "            break\n",
        "    \n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuDme1XvCRQ4",
        "outputId": "d069fea2-3c37-43ca-c1f7-7c71869c5d3f"
      },
      "source": [
        "sample_images = list(sample_train.keys())\n",
        "photo_id = sample_images[5]\n",
        "print (photo_id)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "228822815.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c_5uFIf7CaQO",
        "outputId": "e71746e0-ef9f-4944-c5e3-650ef986b657"
      },
      "source": [
        "photo = train_img_encoding[photo_id]\n",
        "greedySearch(photo)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'two people running in the beach'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9NmYPWbDegc",
        "outputId": "34688895-512e-46c0-965e-05ad65613b95"
      },
      "source": [
        "len(sample_images)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTI8zYbgFmjQ",
        "outputId": "4ef3206d-f569-44ab-df44-52721eed3c21"
      },
      "source": [
        "photo_id = sample_images[49]\n",
        "print (photo_id)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2937611480.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uv1czFHqGGZ_",
        "outputId": "3605e3c8-16eb-4acc-b003-0f16d3a49b82"
      },
      "source": [
        "photo = train_img_encoding[photo_id]\n",
        "greedySearch(photo)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'people people on bench bench bench bench'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbEF4rbHG6rh",
        "outputId": "d34f2f7f-4488-49dc-c30e-534e25664951"
      },
      "source": [
        "photo_id = sample_images[0]\n",
        "print (photo_id)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17516940.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XDLZ2hWeHH8H",
        "outputId": "9b0a339c-40a9-4ddf-e5a2-b5278b42be57"
      },
      "source": [
        "photo = train_img_encoding[photo_id]\n",
        "greedySearch(photo)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lady holding holding in kitchen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtCfjXGoHKI0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}